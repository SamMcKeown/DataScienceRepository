{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8e4601",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "A specialised type of neural network model designed for working with two-dimensional image data.\n",
    "\n",
    "Central to the neural network is the `convolutional layer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479c714",
   "metadata": {},
   "source": [
    "### Sections <a class=\"anchor\" id=\"sections\"></a>\n",
    "- [1. Getting Started](#section1)\n",
    "-  [2. Build A DataFrame](#section2)\n",
    "    - [2.1 Retrieve image filepaths](#section2.1)\n",
    "    - [2.2 Retrieve image labels](#section2.2)\n",
    "    - [2.3 Create a panda series](#section2.3)\n",
    "    - [2.4 Create DataFrame](#section2.4)\n",
    "- [3. Train, test split](#section3)\n",
    "- [4. Load image data](#section4)\n",
    "    - [4.1 Image data augmentation](#section4.1)\n",
    "    - [4.2 Image flow](#section4.2)\n",
    "- [5. Training the model](#section5)\n",
    "    - [5.1 Input layer](#section5.1)\n",
    "    - [5.2 Convolutional layer](#section5.2)\n",
    "    - [5.3 Pooling layer](#section5.3)\n",
    "    - [5.4 Output Tensor](#section5.4)\n",
    "    - [5.5 Flatten output](#section5.5)\n",
    "    - [5.6 Global average pooling](#section5.6)\n",
    "    - [5.7 Dense layer](#section5.7)     \n",
    "-  [6. Create model](#section6)\n",
    "    -  [6.1 Compile model](#section6.1)\n",
    "    -  [6.2 Fit model](#section6.2)\n",
    "-  [7. Evaluate results](#section7)\n",
    "    -  [7.1 Confusion matrix](#section7.1)\n",
    "    -  [7.2 Classification report](#section7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effc530",
   "metadata": {},
   "source": [
    "### 1. Getting Started <a class=\"anchor\" id=\"section1\"></a>\n",
    "\n",
    "`os.listdir()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab062815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Disables unnecessary tensorflow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3737cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sam/Desktop/ttt/imageClassification\n",
      "['.DS_Store', 'images', '.ipynb_checkpoints', 'imageClassificationProject.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Save current working directory to 'cwd'\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# Returns a list containing the names of all entries in the directory given by path\n",
    "files = os.listdir(cwd)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fdbc8",
   "metadata": {},
   "source": [
    "### 2. Build a DataFrame <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "`pathlib.PosixPath`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff07cb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n"
     ]
    }
   ],
   "source": [
    "# Define image directory / folder path\n",
    "imageDirectory = Path('images')\n",
    "print(type(imageDirectory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1c747",
   "metadata": {},
   "source": [
    "#### 2.1 Retrieve image filepaths <a class=\"anchor\" id=\"section2.1\"></a>\n",
    "`.glob()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0655ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 \n",
      "\n",
      "images/cat/374.png\n"
     ]
    }
   ],
   "source": [
    "# Returns an array / list containing all filenames in image directory / folder ending '.png'\n",
    "filepaths = list(imageDirectory.glob('**/*.png'))\n",
    "\n",
    "# Check the number of image filepaths\n",
    "print(len(filepaths),'\\n')\n",
    "\n",
    "# View a filepath\n",
    "print(filepaths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad4751",
   "metadata": {},
   "source": [
    "#### 2.2 Retrieve image labels <a class=\"anchor\" id=\"section2.2\"></a>\n",
    "\n",
    "`os.path.split()` `.map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22a78f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('folderName/labelName', 'example.png')\n",
      "('folderName', 'labelName')\n",
      "labelName\n"
     ]
    }
   ],
   "source": [
    "# Filepath example (how do we extract the label?)\n",
    "filepathExample = 'folderName/labelName/example.png'\n",
    "\n",
    "# Using os.path.split()\n",
    "print(os.path.split(filepathExample))\n",
    "\n",
    "# Split this further\n",
    "print(os.path.split(os.path.split(filepathExample)[0]))\n",
    "\n",
    "# Now the label can be isolated (this is how we extract the label)\n",
    "isolatedLabel = os.path.split(os.path.split(filepathExample)[0])[1]\n",
    "print(isolatedLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958d6ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 \n",
      "\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "# We can now map a lambda function with the formula used above to our list of image filepaths\n",
    "labels = list(map(lambda x : os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "\n",
    "# Check the number of labels\n",
    "print(len(labels),'\\n')\n",
    "\n",
    "# View a label\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190754f2",
   "metadata": {},
   "source": [
    "#### 2.3 Create a pandas series <a class=\"anchor\" id=\"section2.3\"></a>\n",
    "\n",
    "`pandas.Series`\n",
    "\n",
    "note: `pathlib.PosixPath` objects need to be converted to string dtype before being converted to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c531d8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 \n",
      "\n",
      "2 \n",
      "\n",
      "['cat' 'dog'] \n",
      "\n",
      "dog    500\n",
      "cat    499\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Returns a one-dimensional ndarray with axis labels\n",
    "filepaths=pd.Series(filepaths, name='Filepath').astype(str)\n",
    "\n",
    "# Number of unique values\n",
    "print(filepaths.nunique(),'\\n')\n",
    "\n",
    "# Returns a one-dimensional ndarray with axis labels\n",
    "labels=pd.Series(labels, name='Label').astype(str)\n",
    "\n",
    "# Number of unique values\n",
    "print(labels.nunique(),'\\n')\n",
    "\n",
    "# Print the unqiue values\n",
    "print(labels.unique(),'\\n')\n",
    "\n",
    "# Print the number count for each unique value\n",
    "print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b245c3d",
   "metadata": {},
   "source": [
    "#### 2.4 Create DataFrame <a class=\"anchor\" id=\"section2.4\"></a>\n",
    "`pandas.concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca60cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/cat/348.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/cat/374.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/cat/360.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/cat/t1.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/cat/t16.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Filepath Label\n",
       "0  images/cat/348.png   cat\n",
       "1  images/cat/374.png   cat\n",
       "2  images/cat/360.png   cat\n",
       "3   images/cat/t1.png   cat\n",
       "4  images/cat/t16.png   cat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>images/dog/d64.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>images/dog/d70.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>images/dog/d235.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>images/dog/d58.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>images/dog/d209.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Filepath Label\n",
       "994   images/dog/d64.png   dog\n",
       "995   images/dog/d70.png   dog\n",
       "996  images/dog/d235.png   dog\n",
       "997   images/dog/d58.png   dog\n",
       "998  images/dog/d209.png   dog"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe = pd.concat([filepaths, labels], axis='columns')\n",
    "display(dataframe.head(), dataframe.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe83a9b",
   "metadata": {},
   "source": [
    "### 3. Train, test split <a class=\"anchor\" id=\"section3\"></a>\n",
    "`sklearn.model_selection.train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19554c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>images/cat/173.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>images/cat/t53.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>images/cat/277.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>images/dog/d236.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>images/cat/t88.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Filepath Label\n",
       "90    images/cat/173.png   cat\n",
       "305   images/cat/t53.png   cat\n",
       "126   images/cat/277.png   cat\n",
       "962  images/dog/d236.png   dog\n",
       "163   images/cat/t88.png   cat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>images/cat/74.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>images/dog/d128.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>images/dog/d243.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>images/cat/286.png</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>images/dog/g28.png</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Filepath Label\n",
       "37     images/cat/74.png   cat\n",
       "726  images/dog/d128.png   dog\n",
       "846  images/dog/d243.png   dog\n",
       "295   images/cat/286.png   cat\n",
       "924   images/dog/g28.png   dog"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splits arrays / matrices into random train & test subsets\n",
    "dfTrain, dfTest = train_test_split(dataframe, train_size=0.7, shuffle=True, random_state=2)\n",
    "display(dfTrain.head(), dfTest.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cb662",
   "metadata": {},
   "source": [
    "### 4. Load image data <a class=\"anchor\" id=\"section4\"></a>\n",
    "\n",
    "#### 4.1 Image data augmentation <a class=\"anchor\" id=\"section4.1\"></a>\n",
    "`tf.keras.preprocessing.image.ImageDataGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ae57df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train generator that pulls training images\n",
    "# - generates batches of tensor image data, performs training, & recycles memory\n",
    "trainGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                 rescale = 1/255, \n",
    "                 horizontal_flip = True, \n",
    "                 width_shift_range = 0.2, \n",
    "                 height_shift_range = 0.2,\n",
    "                 validation_split = 0.2) \n",
    "\n",
    "# Defines test generator that pulls test images\n",
    "# - generates batches of tensor image data, performs training, & recycles memory\n",
    "testGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                 rescale = 1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f3f52",
   "metadata": {},
   "source": [
    "#### 4.2 Image Flow <a class=\"anchor\" id=\"section4.2\"></a>\n",
    "`flow_from_dataframe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da6a1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 139 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training the images\n",
    "# - specifies how the images will be loaded\n",
    "trainImages = trainGenerator.flow_from_dataframe(\n",
    "              dataframe = dfTrain, # dataframe to flow through\n",
    "              x_col = 'Filepath',\n",
    "              y_col = 'Label',\n",
    "              target_size = (224,224), \n",
    "              color_mode = 'rgb',\n",
    "              class_mode = 'binary',\n",
    "              batch_size = 32,\n",
    "              shuffle = True, # True when training\n",
    "              seed = 42,\n",
    "              subset='training')\n",
    "\n",
    "# Validating the images\n",
    "# - specifies how the images will be loaded\n",
    "validateImages = trainGenerator.flow_from_dataframe(\n",
    "             dataframe = dfTrain,\n",
    "             x_col = 'Filepath',\n",
    "             y_col = 'Label',\n",
    "             target_size = (224,224),\n",
    "             color_mode = 'rgb',\n",
    "             class_mode = 'binary',\n",
    "             batch_size = 32,\n",
    "             shuffle = True, # True when training\n",
    "             seed = 42,\n",
    "             subset='validation')\n",
    "\n",
    "# Testing the images\n",
    "# - specifies how the images will be loaded\n",
    "testImages = testGenerator.flow_from_dataframe(\n",
    "             dataframe = dfTest,\n",
    "             x_col = 'Filepath',\n",
    "             y_col = 'Label',\n",
    "             target_size = (224,224),\n",
    "             color_mode = 'rgb',\n",
    "             class_mode = 'binary',\n",
    "             batch_size = 32,\n",
    "             shuffle = False) # False when evaluating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559524cf",
   "metadata": {},
   "source": [
    "### 5. Training<a class=\"anchor\" id=\"section5\"></a>\n",
    "\n",
    "#### 5.1 Input layer <a class=\"anchor\" id=\"section5.1\"></a>\n",
    "`tf.keras.Input` \n",
    "\n",
    "note: In keras, the input layer itself is not a layer, but a Tensor (object that flows between layers are Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ad6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiates a keras tensor \n",
    "inputLayer = tf.keras.Input(shape=(224, 224, 3)) # images are 224x224 pixels in RGB (3 channels)\n",
    "display(inputLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37cb30",
   "metadata": {},
   "source": [
    "#### 5.2 Convolutional Layer<a class=\"anchor\" id=\"section5.2\"></a>\n",
    "`tf.keras.layers.Conv2D` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef95df0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 222, 222, 16) dtype=float32 (created by layer 'conv2d')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2D convolution layer (e.g. spatial convolution over images)\n",
    "# - the covolutional layer slides a filter/kernal/window across an input image\n",
    "# - as the convolution window slides across the input image\n",
    "# - it sends a new value to a 2D array, resulting in a 2D feature\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                           filters=16, # how many times window slides across an input image\n",
    "                           kernel_size=(3, 3), \n",
    "                           activation='relu')(inputLayer) \n",
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4114ee",
   "metadata": {},
   "source": [
    "#### 5.3 Pooling layer<a class=\"anchor\" id=\"section5.3\"></a>\n",
    "`tf.keras.layers.MaxPool2D` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d6b1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 111, 111, 16) dtype=float32 (created by layer 'max_pooling2d')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MaxPool layer\n",
    "# - maxpool layer reduces the dimensionality of the Tensor\n",
    "# - it sits between convolutional layers & slides a window across the input image\n",
    "# - reducing its dimensionality, enabling the subsequent layer to look at high level features\n",
    "# - that is, the first  layer will view all the pixels of the original image before it is maxpooled\n",
    "# - then the second convolution layer will be presented with only the most important pixels\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "display(x)\n",
    "\n",
    "# 2D convolution layer II\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                           filters=32, \n",
    "                           kernel_size=(3, 3), \n",
    "                           activation='relu')(x)\n",
    "\n",
    "# MaxPool layer II\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=(2,2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5473e3b",
   "metadata": {},
   "source": [
    "#### 5.4 Output Tensor<a class=\"anchor\" id=\"section5.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e3b230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 54, 54, 32) dtype=float32 (created by layer 'max_pooling2d_1')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output Tensor\n",
    "# - Tensor shape is (54 x 54 x 32)\n",
    "# - that is, there are now 32 features assumed useful for making classifications\n",
    "# - however, the output tensor is in 3D form & needs to be compressed down to one dimension\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253703f5",
   "metadata": {},
   "source": [
    "#### 5.5 Flatten output<a class=\"anchor\" id=\"section5.5\"></a>\n",
    "`tf.keras.layers.Flatten` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "437bbc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 93312) dtype=float32 (created by layer 'flatten')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten the output by taking each row & concatenating them together to create one long vector\n",
    "# - essentially, this will multiply 54 x 54 x 32\n",
    "# - however, this will result in 93,000+ features (likely to include mostly redundent information)\n",
    "# - thus, we will use an alternative method 'GlobalAveragePooling2D'\n",
    "outputFlattened=tf.keras.layers.Flatten()(x)\n",
    "display(outputFlattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1182b92",
   "metadata": {},
   "source": [
    "#### 5.6 Global average pooling<a class=\"anchor\" id=\"section5.6\"></a>\n",
    "`tf.keras.layers.GlobalAveragePooling2D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd7595eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 32) dtype=float32 (created by layer 'global_average_pooling2d')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instead of flattening the output, this will average over the first 2 dimensions (54 x 54)\n",
    "# - this will leave 32 values\n",
    "# - the goal here is for these 32 features to represent something meaningful about the input image\n",
    "# - for example, a key physical difference between cats & dogs (i.e. sharp retractable claws of a cat)\n",
    "# - the goal is to extract information rich features\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9aa0ee",
   "metadata": {},
   "source": [
    "#### 5.7 Dense Layer<a class=\"anchor\" id=\"section5.7\"></a>\n",
    "`tf.keras.layers.Dense` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7549f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# Returns a dictionary containing the mapping from class names to class indices\n",
    "# - 1 is the positive class\n",
    "print(trainImages.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb3875d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_2')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add dense layer to the CNN\n",
    "# - units represent the number of neurons in each layer of the NN architecture\n",
    "x = tf.keras.layers.Dense(\n",
    "    units=128, # dimensionality of the output space\n",
    "    activation='relu')(x)\n",
    "\n",
    "# Add a second dense layer to the CNN\n",
    "x = tf.keras.layers.Dense(units=128, activation='relu')(x)\n",
    "\n",
    "# Output also a dense layer\n",
    "# - only outputs one value, with a sigmoid activation since this is a binary classification task (cat or dog)\n",
    "# - the output here is the probability of the image containing a dog as it is the positive class\n",
    "outputLayer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "display(outputLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905820e",
   "metadata": {},
   "source": [
    "### 6. Create model <a class=\"anchor\" id=\"section6\"></a>\n",
    "`tf.keras.Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "058f23b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_2')>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the input & output layers of the CNN\n",
    "display(inputLayer, outputLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a079fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.functional.Functional'>\n"
     ]
    }
   ],
   "source": [
    "# Groups these layers into an object with training and inference features\n",
    "model = tf.keras.Model(inputs=inputLayer, outputs=outputLayer)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ac7a6",
   "metadata": {},
   "source": [
    "#### 6.1 Compile model <a class=\"anchor\" id=\"section6.1\"></a>\n",
    "`.compile()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65da246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.functional.Functional'>\n"
     ]
    }
   ],
   "source": [
    "# Configures the model for training\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d9e7a",
   "metadata": {},
   "source": [
    "#### 6.2 Fit Model <a class=\"anchor\" id=\"section6.2\"></a>\n",
    "`.fit()` `keras.callbacks.Callback` `tf.keras.callbacks.EarlyStopping()` `tf.keras.callbacks.ReduceLROnPlateau`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5532d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 28s 1s/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.4892\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 26s 1s/step - loss: 0.6916 - accuracy: 0.5214 - val_loss: 0.6911 - val_accuracy: 0.5036\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6861 - accuracy: 0.5143 - val_loss: 0.6873 - val_accuracy: 0.5755\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 26s 1s/step - loss: 0.6769 - accuracy: 0.5786 - val_loss: 0.6862 - val_accuracy: 0.5180\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 29s 2s/step - loss: 0.6557 - accuracy: 0.6411 - val_loss: 0.6821 - val_accuracy: 0.5468\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 23s 1s/step - loss: 0.6448 - accuracy: 0.6411 - val_loss: 0.6916 - val_accuracy: 0.5683\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 38s 2s/step - loss: 0.6491 - accuracy: 0.6411 - val_loss: 0.6922 - val_accuracy: 0.5612\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 34s 2s/step - loss: 0.6312 - accuracy: 0.6571 - val_loss: 0.6696 - val_accuracy: 0.6475\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 31s 2s/step - loss: 0.6344 - accuracy: 0.6554 - val_loss: 0.6844 - val_accuracy: 0.6259\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 25s 1s/step - loss: 0.6326 - accuracy: 0.6643 - val_loss: 0.6760 - val_accuracy: 0.6043\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 25s 1s/step - loss: 0.6448 - accuracy: 0.6375 - val_loss: 0.6759 - val_accuracy: 0.6187\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 24s 1s/step - loss: 0.6283 - accuracy: 0.6750 - val_loss: 0.6771 - val_accuracy: 0.6115\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 26s 1s/step - loss: 0.6266 - accuracy: 0.6661 - val_loss: 0.6795 - val_accuracy: 0.6043\n"
     ]
    }
   ],
   "source": [
    "# Trains the model for a fixed number of epochs (i.e. iterations on a dataset)\n",
    "history = model.fit(x = trainImages,\n",
    "                    validation_data = validateImages,\n",
    "                    epochs = 100,\n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                 patience=5,restore_best_weights=True),\n",
    "                                 tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 patience=3)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e5076",
   "metadata": {},
   "source": [
    "### 7. Evaluate Results <a class=\"anchor\" id=\"section7\"></a>\n",
    "`model.evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19f9dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6757357716560364, 0.6100000143051147]\n",
      "Test Loss: 0.67574\n",
      "Test Accuracy: 61.00%\n"
     ]
    }
   ],
   "source": [
    "# Returns the loss value & metrics values for the model in test mode\n",
    "results = model.evaluate(x=testImages, verbose=0)\n",
    "print(results)\n",
    "print('Test Loss: {:.5f}'.format(results[0]))\n",
    "print('Test Accuracy: {:.2f}%'.format(results[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badeb058",
   "metadata": {},
   "source": [
    "#### 7.1 Confusion Matrix <a class=\"anchor\" id=\"section7.1\"></a>\n",
    "`model.predict()` `confusion_matrix()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36366954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvYAAAMECAYAAADUzGGGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAABYlAAAWJQFJUiTwAAA+jklEQVR4nO3dd5QkV3k34N+rXWmVc84JJMAWKIEAEYTIOecgoo0NBocPbCOCMdjYJlhgGxuDEBmDSCaDhGSSASORg0jKOeew2r3fH9UjjWZnNs7ObF89zzl9arrqVtXbPXtmf3371q1qrQUAABhv6813AQAAwJoT7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeYB1TVYdW1Wer6pKqWlpVrapeNw917Dk6d5vrc3NbfhfAyhDsga5V1cZV9eJRUD6rqq6rqmur6vSqOr6qnllVG813nROq6g5JTk7yyCRbJbkkyYVJrpnHssZGVZ0xEYCr6scr0f5tk9q3qtpzFmu5f1W9rqoeO1vHBFiehfNdAMDaUlWPSvKuJDtOWn1tkqVJ9hw9npDkH6rqWa21r811jdN4UZKNk3wjyaNba1fMYy2Lk5w2j+dfU79fVXdrrf1wuo1VtTDJ09fi+e+f5LVJ3pfk02t4rHH/XQBzQI890KWqOipDmNoxQyB6VpJtW2ubttY2T7Jlkidm6B3fOcl956POadxltPzYPIf6tNbOba3t31rbfz7rWE1njZbPXk6bhybZPsmZa7+cNTPmvwtgjgj2QHeq6q5J/j3D37gvJDmwtfbB1tqlE21aa1e21j7RWjsiyVOTXD0/1S5jYliQoTdr5kNJWpKnVdWCGdpMhP4Pzk1JAGuXYA/06A1JFiU5N8nTW2vXL69xa+2/krx16vqqWlRVf1ZV362qK6vq+qo6rareWlU7TnOoVNVRo7HaJ4+eP6qqTqqqK6rqmqr6TlU9bZr9zhhdGHn/0ar3Thr3fcakdssdC768iyyrar1RfSdV1aVVtbiqLq6qn1XVsVX10JU91qQ2B1bVB6vq7Kq6cXTB75er6gnL2WdiHPz9q2rr0ft5+mj/c6vqP6tqp5n2X0lnJfmfDN/YPHiaGrZM8qgk1yU5fnkHqqr7VtUxo38H51XVTVV1UVV9qaqeOE37PUfv2WtHq54zZRz/Lb+/qe9xVR02uvbj/KpaUlX/PF27Sed6yOgC66VVtczrHLX5q9G+V87mNQTAuscYe6ArVbVLkkeMnr69tXblyuzXWpsamLZL8uUkB45W3ZjkpiR3HD2OqqqHt9a+s5xaXp3k9RnG9F+dZJMk90jy4araobX2z5OaX5xkwyRbJ1k/yVVJrp+0bTZ8ILcdU35lks2TbJvkzqPHl1b2YFX1oiTvzK2dRFdkGOL04CQPrqoPJjmqtbZkhkPsmuS4JHtkCNgtw7CoFyR5YFUd1Fq7fGXrmcYHMnxQenaSL07Z9uQM7/eHs5xvR6pq0wwfECZcneH3sl2ShyR5SFW9q7X2B5PaLMlwwfOmGX7nN2R4rzOlzdRzPSXDtwcLR+1net9u0Vr7clX9a5KXZPgw+PuttcsmHfPAJH8zevqy1toZKzomML702AO9uX+SGv3832twnPdnCPWXZwiBm4zG5h+a5CcZZqz5dFVtO8P+d8vQY/vqJNu01rbM0Hs80Tv891W19UTj1tqhrbUdk3x7tOplrbUdR49D1+B1JBl6nTOE+iVJ/jTJ5qOaNswQpo9K8s1VON69cmuoPz7Jbq21rTIE+6MzhPRnJvmr5RzmHRne33u11jbJEIQfk+EDwp4r2HdlfDxDCH9MVW0+ZdvEMJz3r+AYSzO8vsdl+D1u3lrbIsPv/yUZPhS8qKqeNLFDa+3s0e/yzaNV/zXpdznxOHuac707yWeS7DX63Wyc5J9X4nW+IskvM/we/31iZVVtmOGDwvpJPtlaO24ljgWMMcEe6M2dRssbs5qziFTVfTJcWJkkT2utfXyi17m19v0kD8oQSHdI8iczHGaLJK9trb1h4iLY1tqFGQLlRO/8I1envtV02Gj51dbaP7fWrh7V1Fpr57fW3tda+4tVON7fZvg/5FtJntpaO2d0vGtaa29M8qZRu1dOE6on3Jjkga21/x3te3Nr7b8zDKVKhoubV9voNX46w3ULtxyrqvZOcu8k5yc5YQXHuK619qTW2qcn94S31q5orf1rkj8arfqj6Y+wSn6U5MkTveqj9+OMFe00Gmr2jAwz5zypqp412vSmDN/CXJBhtiWgc4I90JttRsvLpw6vWQUTIfD7rbUvT904CugTPaNPnuEYN2Sa3tZRCJs45u+tZn2r46rRcvuqWqO//aNvGo4YPf37GYba/EOG92DTJA+f4VDvmnxB8ySfHi33qqpN1qTW3NojP3l2nImfP7ScYUIr67Oj5WHLuUh3Zb2ltbZ0dXZsrZ2aW8f0/0tVPS+3fuh83gzvM9AZwR5gWQeNlictp83EnPd3nCF8/ry1du0M+547Wm61OsWtphMzXCNwUJKTa7gx186reawDMwx3arnt+PNbjK5tOGX09KDp2iT5vxnWnzvp5y1Xo77JvpqhZ/6+VbXHaN1Ej/aKhuEkGea7r6rnjy6WPX90ke/EhawT1wBsmDX/ff7vGu7/DxmGU22e5D0ZfkfvbK1Nvb4A6JRgD/Rmomdyq6qq5bac2Xaj5bnLaXPOaFkZLj6dannTZ94wWq6/inWtttbar5O8OMOY8/tkuLD03NFsNO8cXWS5sibenytba8ublnPiPdpuhu3TvkettRsmPV2j92jUI//hDL+nZ1bV4Un2TvLD1tpPVrT/pItn353hYtkdM1yncHGGC2QvnNR8Tb9dWKOLpEe9/S+YtOqMJKsyvAoYc4I90JtfjJaLkuy3hsfacA33X6e01o5NsleSl2e4SPPSDBep/mGSU6rqr1fxkItms761aKJn/llZ+YtmJ7w6yb2SXJLkOUl2aK1t3FrbfnSB7C6T2q7uB8kkt3wIWVPPnfTzTkn2mYVjAmNCsAd68z8ZhogkyaNX8xgTPae7L6fNrqNlyxD65spE+JvpQ8cWy9u5tXZha+2Y1tpjM/Sk3z3JpzKE0r+tqgNWooaJ92ej0bSgM5l4j2Zrus7V0lr7cZIfZ/igd1SG9/DDK7n7xGw3L22tvb+1dtGU7TvMSpGzYHTR9/8bPf1phg9eH6yqDeavKmAuCfZAV0azs3xh9PSly5mR5TamDNs5dbS833KG8zxgtPzVcsbSrw1XjJa7zrB9pafGHM2I838Zwus5Gf5POHwldv1Bbv3wdMR0DapqiyQHj56eOl2bOTbRQ79+kq+MLoBeGRPv8w9m2P7A5ew7cSHsGvXkr4zRv/P3Z/gdHpvh3+dFSQ7IrbMMAZ0T7IEeHZ1hKsVdM9wMarlDaqrqyUn+bNKqibnm75JhXvWp7XfIMHwlST62xtWumolx4dPVtSjDMJtlLK/XdjQEZPHo6QqH14ymfZy4sPiVM8yy88oM3ypck1s/aM2nDyR5y+jxxlXYb+LGUr8/dcNo/P2rlrPvxExEW67C+VbX2zMMqzo9yctbaxcneeFo25+P7mMAdE6wB7rTWvthkj/O0Kv8iCQ/GM0Cc8sNoapqi6p6fFWdlOS/kmw2af9v5NY7sB5bVU+cmMqwqg5O8pUMM6BcmOSYOXhJk018kHhhVT13FOZTVXfJEKBnmunm76rq+Kp67JT3YYeqenuGsfctwywyK+PVGXqkD0ry0aradXS8TUdj9f9y1O5NrbWrZjjGnGmtXdRa+4vR41ursOvE+/HWqrrlG5yqOjTDTEPbzLhn8rPR8vCqusOqV71yqurxGcb/L03yrEn3KPjvDLPjrJfkfSv77RUwvgR7oEuttfckeXyG4Qj7Z+ixvbSqrq6qqzIMaflEhjvVnplbp6+c8OwkP8wQ4D+e5JrRft/PMLzh8iSPm4f5wd+d5LsZetaPHdV1ZYYx1XfLbS+enGxhkidkGE9/aVVdOXo9FyR56ajN0a21n65MEa21b2e4KdPSDEN5zqqqyzK8r2/MMPzkQ7n1RlXj6ugM11DsluTkJNdV1TVJvpehF//py9n35CS/TbJ1ktOq6qKqOmP0mGko1Sqpqh2TvGv09B+n+dDy8iS/y9Cb//bZOCew7hLsgW611j6dYWrDP87Qm31OhoC7MMNUgMdnCGb7tda+PmXfi5PcM8N0gd/PMFRlgyS/znDjqbtM3DF1LrXWFme48+0/ZXgNS5Ncm+S4DGPafzTDrm/LcMOizyT5VYbgvSjJ2Rm+sbhva+3vVrGW/8gwpv/DGeaK3zTD0JWvJnlSa+2ZszTTy7xprf0uwwXGH8zwIXFBhg8vH0pyaGvtK8vZd3GSIzOaWjTDh8Q9Ro+Fs1TisRm+Nfhhbr1B1eQarsnwIXVpkueMeveBTtXq35gRAABYV+ixBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6sHC+CxgXGx34kjbfNQCMm8Of/8z5LgFg7Hz1JYfV6uynxx4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAfGKthX1Wuq6r7zXQcAAKxrxirYJ3ldkvvPcw0AALDOGbdgDwAATEOwBwCADgj2AADQgYXzXcBq2HNVL6BtrX19bRUDAADrgnEM9s8ZPVZWy3i+TgAAWGnjGHjPSnLGfBcBAADrknEM9u9trb1+vouAufDcx90rz33cvXKnfXZKVfLL0y/McZ/6dt7ziW+ltXZLuwPuuEsedcRdc+Rh+2XPXbbNNltukksuvybfPPU3edv7TsgPf3nOPL4KgLnzgWcfmB03XzTttsuuvSlPee+ptzz/f0fukwffabvlHu8HZ1+ZV3zmF7NaI6wt4xjs4XbhvW98Tp768ENz4aVX5WNf+n6uv+GmPOCw/fOOVz01h911r7zg1R+4pe07XvXU3P2AvXLKz8/KZ772o1x73Y05YL9d8+SHHpLHHXlgnvWXx+YzX/vRPL4agLlzzY0355M/umCZ9dcvXnKb59/63WW54Oobpz3GA/fbNjtvsWG+d9YVa6NEWCsEe1gHPfqIA/LUhx+a08+5JPd51j/l0iuuTZKsv3BBPvLmF+QZj7xHPnvSj28J6x/94vfz3KPfl9+dfcltjvPUhx2S9/7dUfmXo5+WL3z9p1l885JlzgXQm2tuXJIPfG/F31R++/TL8+3TL19m/SYbLMiTD9wpNy1Zmq/84uK1USKsFV1Pd1lV61XVY+a7DlhVj37AXZMkx3zga7eE+iRZfPOSvP7fPpck+cOn3Do51Ds/+j/LhPpkCPy/PvOibLvVpvm9O+y8lqsG6MMD9982G66/IN/67WW56oab57scWGnj1mN/ZpIrVtSoqvZI8oIkz02yU5IFa7csmF07bLN5kuT0c5cN66efe2mS5N4H7pv1Fy5YYS/8xPablyyd5SoB1k3rL6gcecdts/1mG+SGxUvzu0uvy0/OuypL24r3TZKH33n7JMnnf3bRWqwSZt9YBfvW2l4zbauqBUkek+RFSR6Y4duIluSEuakOZs9EL/2eO2+zzLa9dhnWrb/+guy167b51RkXznicu//+nrnzPjvl3Asvz89+c97aKRZgHbPNJhvkLx+8723WnX/lDXnzib/Nj8+7ern73mnHTbP3tpvk7Muvz4/OvWptlgmzbqyC/XSqau8kL0xyVJLtR6svSfIfSd7TWjtznkqD1falb/w0T3nYIfmTZz4gH//yKbn8quuSJAsXrpejX/yIW9pttfnGMx5jq803zrv/9tlJkle8+ZNZurJdVQBj7Mu/uCg/Pe/qnHHZ9bl+8ZLstPmiPOaAHfPwu2yfNz5q/7zs+J/ld5deN+P+j7jLECW+8HO99YyfsQz2VbUwyeMy9M4fkaF3/qYkn0zyhCSfaa29ZjWOe8pM2za82x+vXrGwGj725VPytEfcPQ++951z6ieOzudO/nFuvGlxjrjH/tlx281z1vmXZfedts7SpdMPr9l4ww3y8be9KHfYY/u85b1fzSdP+MEcvwKA+fHB/zv3Ns/PuOz6HHPy6bl+8ZI86cCd86y775q/+eKvpt134w0W5L77buOiWcbWWF08W1V3qKp/THJuko8mOTLJD5K8NMlOrbUnzWd9MFuWLm15wsv/PUcf8+lccvk1eeaj7pFnPOoe+e1ZF+WIo96aq6+9IUly0WXXLLPvxhtukE+948W590H75pgPnJij3/6ZuS4fYJ3zuZ8OwxYP2GWzGds8cL9ts5GLZhlj49Zjf1qGcfMXJnlrkuNaaz+brYO31g6eadtGB77EOAbm1M03L81bjjshbznutpeJLNpgYfbdfbtcfPnVOfO8S2+zbdONF+VT73hxDj9o37zlvV8V6gFGrrh+COobLpx5Po2Ji2Y/56JZxtRY9diPtCRfTPKJ2Qz1MC6e9JCDs2iD9fPxL9125Njmm26Yz73zJTn8oH3zpv/8klAPMMmdd9w0SXL+VTdMu33/HTbNPtsNF83+2EWzjKlxC/avTnJWhmksv1VVP6+qV1TVTvNcF8y6zTbZcJl1B9xxl/zdnz42l115bd783q/esn7LzTbKF/79pbnHAXvl9e/8XP5mNNc9wO3J7lttmA0XLhttdthsUV5y3z2TJCeetuw0wkny8ImLZvXWM8bGaihOa+2NSd5YVQ/JMBPOo5K8abTuK0neN5/1wWz6/DtfkutvXJyf/+a8XH3djdl/rx3y0MN/L9ffeFOe8PL/yPkXX3lL24++5YU5+C575LdnXZz1ar286g8evszxPnvSj/LjX527zHqAXtzvDtvmiXfbKT8576pcePWNuf6mpdlpi0W5x55bZdHC9fLdMy7Px39w/jL7bbz+gtx/321y081L85VfumiW8TVWwX5Ca+3LSb5cVdsneV6Gm1E9LMlDMwzVuVtVHdxam3GWG1jXfeqEH+SJDzk4T33Eodlo0fo576Ir855PfitvPvYrOfeiK27Tds/R3Pb77L5djv7DZUN9kpx53qWCPdC1H51zZXbbcsPss90muctOm2XDhevlmpuW5KfnXZUTTrskJ8zQW/+A/bbNRhssyEm/usRFs4y1aq2Pa0Kr6sgM018+JskGGQL+j5O8u7X2r2t6fBfPAqy6w5//zPkuAWDsfPUlh9Xq7DduY+xn1Fo7sbX2lCS7JnlFkt8kuWuSt89rYQAAMAe6CfYTWmuXtNbe3FrbL8PNqz4y3zUBAMDaNlZj7KtqgyTfTHJ1koe21hYvp90Xk2yS5D5zVyEAAMyPceuxf2aSg5O8ZaZQnySttZuS/FOSuyd5xhzVBgAA82bcgv3jk/yutfaFFTVsrX0pya+TPGmtVwUAAPNs3IL9gUlOXoX2X09yt7VSCQAArEPGLdhvm+TCVWh/YZJt1lItAACwzhi3YH99kk1Xof2mSW5YS7UAAMA6Y9yC/dlJDlmF9ockOWst1QIAAOuMcQv2Jye5Z1WtMNxX1cFJ7pXkpLVdFAAAzLdxC/b/kqQl+XhV3WmmRlW1f5KPJ1mS5N/mqDYAAJg3Y3WDqtbaaVX1+iSvS/KDqjo+ydeSnDNqskuSI5M8IcmiJK9prZ02H7UCAMBcGqtgnySttddX1c1JXpvk6UmeNqVJJVmc5FWttb+f6/oAAGA+jF2wT5LW2t9V1YeSPC/JvZPsNNp0fpJvJnlva+3M+aoPAADm2lgG+yQZBffXzncdAACwLhi3i2cBAIBpCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANCBhau7Y1X9bjV3ba21fVb3vAAAwLJWO9hn6O1vq7FfrcE5AQCAaax2sG+t7TmLdQAAAGvAGHsAAOiAYA8AAB1YkzH206qqRUkOTbJLkkXTtWmtvX+2zwsAALdnsxrsq+p5Sf4xyVYzNclwwa1gDwAAs2jWhuJU1UOTvDvJ+Un+IkOI/0ySVyX56uj5x5M8b7bOCQAADGZzjP2fJ7k0yb1aa28brftha+1NrbWHJnlhkscn+e0snhMAAMjsBvuDkny2tXb1dMdvrb0nybcy9OADAACzaDaD/SYZhuFMuCHJ5lPafD/JPWbxnAAAQGY32F+QZLtJz89Pst+UNlskWTCL5wQAADK7wf5nuW2Q/0aSI6vqPklSVb+X5MmjdgAAwCyazWD/xST3rqqdR8//McmSJCdX1cVJfpRksyRvmMVzAgAAmd1g/x8Zbkp1SZK01n6e5MgMgf+SJF9J8rDW2hdm8ZwAAEBm8QZVrbXFSS6csu47SR45W+cAAACmN5s99gAAwDwR7AEAoAOzNhSnqpYmaSvRtLXWZu28AADALAb7JF/P9MF+yyR3TLJRhplxrpjFcwIAAJndi2fvP9O2qtosyduS3CvJ42frnAAAwGBOxti31q5O8qIkNyd541ycEwAAbk/m7OLZ1trSJCcleexcnRMAAG4v5npWnA2TbDXH5wQAgO7NWbCvqv2TPCnJb+bqnAAAcHsxm9NdHrucc+yW5N5JFiT589k6JwAAMJjN6S6PWsH2Xyb5p9bae2fxnAAAQJJqbWXuKbUSB6raY4ZNS5Nc3lq7ZlZONE8Of/M3ZueNArgdOeUjx893CQBj5/pTjqnV2W8257E/c7aOBQAArJpZu3i2qo6tqkevoM0jlzMWHwAAWE2zOSvOUUnutoI2d03ynFk8JwAAkLmfx35RkiVzfE4AAOjebAf7GS8wrapFSe6b5IJZPicAANzurdHFs1X1uymr/rSqnjtN0wVJtsvQY//va3JOAABgWWs6K856ubWXviWp0WOqxUl+kuTEJG9Yw3MCAABTrFGwb63tOfFzVS1N8rbW2uvXtCgAAGDVzOadZ49IcsYsHg8AAFhJs3mDqv+ZrWMBAACrZjZvUHV0VS2uqp1n2L5LVd1UVa+crXMCAACD2Zzu8lFJTm6tnTfdxtbauUlOSvLYWTwnAACQ2Q32+yb5+Qra/HzUDgAAmEWzGew3SnLdCtrckGSzWTwnAACQ2Q325yQ5bAVtDkty7iyeEwAAyOwG+y8luW9VPWW6jVX11CT3S/LFWTwnAACQ2Z3H/h+SPCPJh0fh/ksZeud3SfKwJI9OclmSN83iOQEAgMzuPPbnVtVDknw8w8w3j5m0uTLcvOpJrbVzZuucAADAYDZ77NNa+35V3THD1JeHJdkyyRVJvpPks0mWVNVjWmufmc3zAgDA7d2sBvskaa0tTvLJ0SNJUlV7JHlNkucm2SnJgtk+LwAA3J7NerCfUFULMgzHeVGSB2a4ULclOWFtnRMAAG6vZj3YV9XeSV6Y5Kgk249WX5LkP5K8p7V25myfEwAAbu9mJdhX1cIkj8vQO39Eht75mzIMx3lCks+01l4zG+cCAACWtUbBvqrukKF3/jlJts0w+80pSY5L8uHW2uVVtXRNiwQAAJZvTXvsT8swbv7CJG9Nclxr7WdrXBUAALBKZuPOsy3D3WQ/IdQDAMD8WNNg/+okZ2WYxvJbVfXzqnpFVe205qUBAAAra42CfWvtja21vZM8LMmnkuyT5E1Jzqqqz1fVk2ehRgAAYAVmYyhOWmtfbq09McluSf46yZkZwv5HMgzVuVtVHTwb5wIAAJY1K8F+Qmvtotbam1pr+yZ5UJLjkyxOckiS71XVD6rqj2fznAAAwCwH+8laaye21p6SZNckr0jy6yR3TfL2tXVOAAC4vVprwX5Ca+2S1tqbW2v7J3lAhuE5AADALJqVO8+urNbayUlOnstzAgDA7cFa77EHAADWPsEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdGDsgn1V/UNVHVNV601a99qqWjLN42vzWSsAAMyVsQr2VXW/JH+R5KzW2tKpm5OcN+lxcZL7VdV95rZKAACYewvnu4BV9OQk1yb5j2m2tdbabhNPqmrjJBckeWqSb8xNeQAAMD/Gqsc+yT2TfKO1ds2KGrbWrkvylST3WutVAQDAPBu3YL93ktOmWV+jx1TnJNlzbRYEAADrgnEL9htnGIoz1duS7DXN+mtG+wAAQNfGbYz9lUm2n7qytXblaNtU2ye5em0XBQAA823ceux/neTwVWh/79E+AADQtXEL9icm2b+qHrmihlX1iCR3SnLCWq8KAADm2bgF+3cmuTHJcVX1gJkaVdURSd6X5IbRPgAA0LWxGmPfWjuvql6e5N+TfLWqvp3ka0nOHTXZOcmRGaa4rCQvaq2dNx+1wmw5ePct84QDd8pddt48my1amKtuWJzfXnxdPn7qufnO6Zffpu3v7bxZnnPY7rnLTptl0cL1cvYVN+TzP7kgn/jBeVna5ukFAMyx5z7unnnuYw/LnfbeKVXJL0+/MMd9+jt5zye/ndZu/WO4xaYb5XmPu2cO2G+X3HW/XXOH3bfLwoUL8vAX/2tO+t6v5vEVwOoZq2CfJK21d1XVDUn+OcMY+qnz1FeSK5K8vLX2/rmtDmbXi++7Z55x991y4VU35lu/uTRXXL84W268fvbbYbMcuNsWtwn2h++zdd7wmDvnppuX5munXZyrbrg5995767zsAfvkgF02z6s/+8t5fCUAc+O9b3hWnvqwQ3LhpVfnY18+JdffsDgPuMd+ecdfPzmHHbBnXvDaD93Sdo+dt87fvfwxSZJzLrg8l1xxbXbcdvP5Kh3W2NgF+yRprb2/qj6d5IkZwv2Oo00XJPlWkuNba1fNU3kwKx71+zvmGXffLV/46YX5x6/8OjdP6XJfsN6tt27YeIMFeeVD7pClS1te+l8/zmkXDvdwe/c3z8gxTz4gR+y3XY781aU58bSL5/Q1AMylRx9xQJ76sENy+jmX5D7PeWsuvWKYIXv9hQvykX96Xp7xyLvnsyf/JJ856cdJkrPOvywP+8N/zY9OOyeXX3Vd3vW6p+dZj7rHfL4EWCPjNsb+Fq21q1prx7bWnt9ae8To8fzROqGesbb+gsqL7rNHLrjqhmlDfZIsmbTuiDtum6023iAnnnbxLaE+SW5a0vKf3zwjSfLYu+201usGmE+Pvv/vJ0mO+eBJt4T6JFl885K8/p1fSJL84VPuc8v6K66+Pif/369y+VXXzW2hsJaMZY899O7QPbbKVhtvkP/6/rlpreWee2+VvbfdJDfdvDQ/P//q/Oz8296e4aDdt0ySfHfKmPsk+dE5V+b6xUvy+ztvlvUXVBYvMdge6NMOo2E0p5976TLbTj/3kiTJvQ/cJ+svXJDFNy+Z09pgLoxtsK+qw5L8QYahODuPVp+X5JtJ/rO19r/zVRusqf133DRJctOSpTn22Qdln+02uc32H5x9ZV7937/IFdcvTpLsvvVGSZKzL79+mWMtacn5V96QvbfdJDtvsWHOvGzZNgA9mOil33OXbZbZttcu2yYZhuXstes2+dUZF81pbTAXxm4oTlWtX1X/mWEs/XOS7Jtk49Fj3yRHJflmVf1nVa0/b4XCGthq4w2SJE87dNe0JH/0kR/lQcd8K88+7pR89/TLc+BuW+RvH73/Le033WBBkuSaG2+e9ngT6zddNLaf5QFW6Evf/FmS5E+ecf9stfnGt6xfuHC9HP0HD7vl+VabbbzMvtCDcfxf/j1Jnpnk8iTvTvLVJGePtu2W5EFJXpDkeUk2yBD+YaxMXBe7ZGnLX37qZ7ngqhuTJL+75Lr89Wd+ng8/75AcuNuWuctOmy0zLAfg9upjXz41T3v4oXnwve6UUz/+V/nc//wkN950c464+x2z47ab56zzL8vuO22dpc2QRPo0Vj32VfXADKH+lCR3aa29srV2QmvttNHjhNbaK5PcOcmpSZ5ZVUeuwvFPmemxdl4RTG+ih/3XF11zS6ifcOPNS/O9M4ax9HfeabOh/U3DWNGZeuQn1s/Uow/Qg6VLW57wp+/K0W//71xyxTV55iPvnmc88tD89uyLc8Tz/jlXXzv8Pb3oMh0i9GnceuxfmOTaJI9trV0wU6PW2oVV9dgkp432OXFuyoPZcdZoHPxMQfzq0fpFC9e7pf2ddtwsu2210W1mxUmSBZXstMWGuXnJ0px35Q1rsWqA+XfzzUvzlvedmLe877b/9S/aYGH23X27XHz5NTnzvMvmqTpYu8aqxz7JPZN8sbV27ooajtp8PsvewGp5+xw802MNaoZV9v2zrsjS1rLnNhunptm+97bD+NCJoH7qWVckSe6x11bLtL3rrltko/UX5CfnXW1GHOB260kPOSiLNliYj3/Zl/D0a9yC/fZJfr0K7X+TZLu1VAusNRdedWO+9dvLsuPmG+ZJB+98m22H7rFl7r7nVrn6hsW3TG950q8uyeXXLc6R+22X/XbY9Ja2GyyovPDwPZMkn/7h+XNWP8B82WyTRcusO+COu+TvXvaYXHbltXnze0+Yh6pgbozbUJzrkmy2Cu03S2LsAWPprSf8JnfcfpP8yRH75F57b51fXXRtdtpiw9xn322ydGnLm77861w7Glt/3U1L8o9f+XX+9tF3yjueckBO/OXFueqGxTl8n22yxzYb56TTLnbXWeB24fP/9ke5/obF+flvz8/V192Y/ffaIQ89/C65/obFecKfvivnX3Lbe1j+/csfk222HKYUvtfd9k6S/OmzH5CnPfyQJMlnT/5JPnvyT+b2RcBqGrdg/+skD1iF9kdk1Xr4YZ1x8TU35fkf+GGOuufuOXzfrXPXXbfItTcuybd/e2k+8N2z84sLbjuW/hu/uTQv/eiP8+zDdsv977hNNliwXs654oa8/aTf5vhTz5unVwEwtz51wo/yxIcclKc+/JBstGiDnHfRFXnPJ7+dN7/3qzn3oiuXaf+4I++aPXa+7bz3D7rnnW75+czzLhPsGRvVxmjKp6p6TZLXJnlJa+2dK2j7h0n+NcnftNZev6bnPvzN3xifNwpgHXHKR46f7xIAxs71pxwz3SV2KzRuY+zfnuSyJG+vqjdU1RZTG1TV5lX1t5PavmOOawQAgDk3VkNxWmtXVNUTMsx281dJ/mw0x/zkG1QdnGRRkuuTPLG1dvm8FAsAAHNorIJ9krTWvl5V98jQI/+AJPeeptlJSV7WWvvpnBYHAADzZOyCfZK01n6e5IFVtWeSw5PsNNp0QZJvttZOn6/aAABgPoxlsJ/QWjsjyRnzXAYAAMy7sQ72SVJVe2S4CVVLcnFr7ax5LgkAAObcuM2KkySpqm2r6q1VdX6S3yX5bpLvJTm9qs6rqn+qqq3nt0oAAJg7Yxfsq+oOSb6f5GVJdkiyJMlFSS4e/bxjkj9L8v2q2nu+6gQAgLk0VsG+qtZL8qEkuyf5nyQPTLJpa22n1tqOSTZL8uAkX0+yZ5IPzlOpAAAwp8Yq2GcI7Yck+ViSI1trX2ut3TSxsbV2Y2vthAzTYB6f5B5V9aD5KRUAAObOuAX7JyS5MclLW2ttpkajbS9JsjjJE+eoNgAAmDfjFuwPSvKt1trFK2rYWrsoyTdH+wAAQNfGLdjvluRnq9D+Z0n2WEu1AADAOmPcgv3mSa5YhfZXZLigFgAAujZuwX6DDFNarqylo30AAKBr4xbsk+EOswAAwCQL57uA1fC6qnrdfBcBAADrknEM9rWK7fXwAwDQvbEK9q21cRw6BAAAa52gDAAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA9Vam+8agDVQVackSWvt4PmuBWBc+NtJj/TYAwBABwR7AADogGAPAAAdEOwBAKADgj0AAHRAsAcAgA6Y7hIAADqgxx4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPawjqiq/avqHVX106q6sqpuqqrzqurzVfX8qlo0w36vqqo2euw3ZdsZk7atzON1c/JiAVbBNH+rbqyqi6vq1Kp6d1U9rKoWrOAYW1XVa6rqe1V1+egYZ1fVx6rqQStRwy5V9bdV9b9VdUlVLR79rf5BVf1bVd1/tl4vrC43qIJ1QFW9JslrM3zY/t8k309yTZIdktw/yd5JTmmtHTJlv0ryuyR7JKkkb2mt/cWk7S9PsuWU0x01av++JGdM2XZya+3kNX5BALOoqibCyt+Mlgsy/G27S5J7J9kgw9/NZ7TWfjXN/vdN8okk2yb5RZKvJbk6yR2SPCzJxkk+mOQFrbUbp9n/RUmOSbJhkl8m+WaSi5JskmS/JPcZ/fzm1tr/W+MXDKtJsId5VlV/neSNSc5O8qTW2nenafPIJH/eWjtiyvqHJPlSkuOSPDTJwiS7tNZuWs75Tk5yvyRHCPHAOJgI9q21mmbbDknekeRJGf6OHtJau2jS9jsn+V6SjZK8LMm/tknhp6p2S/LpJAclOba19vwpx392ho6Qy5M8t7X2mWlq2DzJHyTZvbX20jV6sbAGBHuYR1W1Z5KJ3qWDWms/XU7bRVN7kqrq+CRPyNBj9fgkf57kqa21/1rOcU6OYA+MkeUF+9H29ZKcmOEbzmNaay+ftO2EJEcmeVNr7a9m2H+nJD9LslWSe7fWvj1av0WS00frH9xa++oK6lzYWrt5lV4czCJj7GF+PTfJ+kk+sbxQnyTThPodkjw6ya9G/wkdN9r0orVQJ8A6q7W2NMkbRk+fNhqmmKraK0OovzHJPy5n//OTvHv09A8mbXpihlD/7RWF+tFxhHrmlWAP8+vw0fLE1dh34kPBcUky+mBwSpIjqmrfWakOYHx8M8nNSbZPsudo3cTf2FNaa5evYP+J4H7vSesmfv7abBQIa9vC+S4Abud2Gi3PWZWdRr1RL0iyNMn7J206LsnBSV6Y5JWzUB/AWGit3VhVl2aYdGC7DENoJv7Gnr0Sh5hos/OkdTuOludObVxVWyZ5+TR1vG6lCoa1QLCH8fSAJPsk+XJrbfJ/OB9O8pYkR1XV0a21xfNSHcD8mBiDPxcXEG6ZYTazqV43B+eGaRmKA/Pr/NFyl1Xcb2Ic/XGTV7bWLkvy2QxfRT9mjSoDGCNVtWGSrUdPLx4tLxgtd1uJQ0y0OW/Suon9d57SNq21M1prNfHINL36MNcEe5hf3xwtj1zZHapquySPHT39yNQbt2SYJSdxES1w+3J4hpEIF7bWzhitm/gbe/Bo6MzyPHC0/NakdRM/r/TfaJhPgj3Mr/cmWZzkCaO5lmc06c6zz8lwM5ZTkrxnhsfFSR44mhECoGuj6S5fNXr64Yn1rbXfJTkpyaIkM944ajTL2AtHT981adPxSa5Icq+qEu5Z5wn2MI9GvUqvyxDUP19Vh0zXrqoemuSLo6cT//n8UWvtBdM9kvxHhrGmL1irLwBgnlXV9kk+mmEO+7OS/N2UJi9Lcl2SV1bVi6fZf5ckn8swreV7W2u39Ni31q7MrRfIfmx0s8Dpatg4wyxlMK/coArWAVX1mgwXYa2X5NsZbo1+TYbZHe6b4bbn38/Q43RSkp+01g5YzvH2TPK7DONDd588t7IbVAHjZuIGVUn+ZrRcL8PFq3fJMARngwx3l31Ga+030+x/RIbe960z3IjqpCRXJ9k3ySOSbJzkQ0meP/WeIaP9X5Tk7Rl6/n+RYYjORUk2TbJrkgePfv56a+1+a/yCYTUJ9rCOqKo7JfmjJEck2T3JhkkuTfLDDP8hfTDJsUmenuRlrbW3r+B4X0nyoCSPb619atL6kyPYA2NkUrCfcFOGYH5mklOTfCLJV0Y3qprpGNskeWmSR2boLNkww7DF/03yn621r6yghl2TvDjDWPw7JNkswzcBZ4yO8ZHW2v+s6muD2STYAwBAB4yxBwCADgj2AADQAcEeAAA6INgDAEAHBHsAAOiAYA8AAB0Q7AEAoAOCPQAAdECwBwCADgj2AADQAcEeAAA6INgDsMaqqlXVyVPWvW60/v7zUtQqGrd6AaYS7AHGxCh0Tn4sqapLquprVfX0+a5vbZjuAwMA01s43wUAsMr+ZrRcP8n+SR6T5IiqOqS19mfzV9Yy/iXJR5OcNd+FANweCPYAY6a19rrJz6vqyCRfTfLyqnp7a+2M+ahrqtbaJUkume86AG4vDMUBGHOttROT/DJJJTk0ue148ap6elV9t6quqaozJvarqo2r6q+q6odVde1o+/9W1dOmO09VbVBVr66q31bVjVV1elW9oaoWzdB+xjHrVbV/VR1bVWeMjnVRVX2jql482n5UVbVR8/tNGYL0uinHukdVHV9VF1TVTVV1dlX9R1XtPENdB1fVl6rq6qq6qqpOqKp7Lv9dBlj36bEH6EONlm3K+j9P8qAkn01yUpItkqSqtkzytSQHJjk1ybEZOnsekuTDVXWX1trRtxy8qpJ8LMOwn99mGGazQZLnJfn9VSq06hFJPp5kUZIvJflIki2T3DXJK5K8M8kPMww5em2SM5McN+kQJ0861vOSvCvJjUn+O8nZSe6Q5AVJHlVVh7XWzprU/l5JThjV/skkv0lyt9Exv7YqrwNgXSPYA4y5qnpgkv0yhPr/m7L5AUnu2Vr7wZT1/5wh1L+ytfaPk461YZJPJ/nrqjq+tfbD0aanZQj130lyRGvthlH7105zzuXVum2SD2f4/+cBrbX/mbJ91yQZnfeHo+OfMXX40ajtHZP8e5IzktyvtXbupG1HJvlKkmOSPG60rjJ8gNkoyWNba5+Z1P5lo/cEYGwZigMwZkZDXF5XVW+squMz9HpXkn9urZ05pfm7pob6qtomyTOTfH9yqE+SUWB/5eh4k2faee5o+dcToX7U/rIkf7sK5T8nyeZJ3jk11I+Od84qHOvFGS4gftnkUD86zokZevAfVVWbjVbfK8MHoK9PDvUj/5LhmwiAsaXHHmD8vHa0bEmuSPKNJO9prX1wmrbfm2bdoUkWJFlmvPrI+qPlnSatOyjJ0iTfnKb9ySus+FaHjZZfXIV9ZjIxLv5+VXXoNNu3z/A675jklAyvIUmm+0CxpKq+mWSfWagLYF4I9gBjprVWK251iwumWbfNaHno6DGTTSf9vEWSy1pri1fyHDPZcrQ8d3mNVtLE6/h/K2g38Tq2GC0vnKHdqrwOgHWOYA/Qt6kX0ybJlaPl21Zh3vsrk2xdVetPE+53XIV6rhgtd0nyk1XYb6aakmSL1tpVq9B+hxm2r8rrAFjnGGMPcPvzvQzDau6zCvucmuH/jMOn2Xb/VTjOd0bLh61k+6UZhtMs71gr+zpOHS3vN3VDVS3I9K8NYGwI9gC3M621i5J8KMkho3nplwnOVbVPVe01adV7R8s3jmbOmWi3dZKjs/Lel+SqJC+uqvtOc95dp6y6NMluMxzrX5IsTvK20Qw5U4+1QVVNDv3fTnJakvtW1WOmNH9JjK8HxpyhOAC3Ty/JMN/765M8a3Th6IVJds5w0eyhGaa4PH3U/iNJnpLk0Ul+WlWfyXCR7RMzTHe5UqG4tXZJVT09yfFJTqqqLyb5cYaZcg7IEOInf6A4MclTq+qzGXrcF2eY1ebrrbVfjuaxPzbJz6rqS0l+Napr9ww9+Rcn2X907lZVz89wl95PVNXkeeyPzDC70ENX6t0DWAcJ9gC3Q621q6rqfklelGFayyck2TBDuP91kj/NEIAn2reqelKSv0xyVIYPBudn6Ml/fZIbspJaa5+vqkMyTKt5ZJIHJ7k8w91z/35K85dluE7gyCQPz/BN898k+froWB+sqh9luBHXEaNjXZvkvAwfHv5ryrm/NerFf2NuHQ703QzDiR4SwR4YY9XadNdVAQAA48QYewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6IBgDwAAHRDsAQCgA4I9AAB0QLAHAIAOCPYAANABwR4AADog2AMAQAcEewAA6MD/B2FuuLW/nY7NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 386,
       "width": 379
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generates output predictions for the input samples\n",
    "# - classification threshold is always 0.5 in a binary classification task\n",
    "# - if the output predictions are >= 0.5, the model will classify the image as being a dog (i.e. [1])\n",
    "predictions = (model.predict(testImages) >= 0.5).astype(np.int)\n",
    "\n",
    "# Performance measurement for machine learning classification\n",
    "confusionMatrix = confusion_matrix(y_true = testImages.labels,\n",
    "                                   y_pred = predictions,\n",
    "                                   labels=[0, 1])\n",
    "\n",
    "# Plot confusion matrix  \n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusionMatrix, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['CAT', 'DOG'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['CAT', 'DOG'])\n",
    "plt.xlabel('Predicted', loc='center')\n",
    "plt.ylabel('Actual', loc='center')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9754ee1",
   "metadata": {},
   "source": [
    "#### 7.2 Classification Report <a class=\"anchor\" id=\"section7.2\"></a>\n",
    "`sklearn.metrics.classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99aab94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAT       0.61      0.62      0.61       149\n",
      "         DOG       0.61      0.60      0.61       151\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.61      0.61      0.61       300\n",
      "weighted avg       0.61      0.61      0.61       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Builds a text report showing the main classification metrics\n",
    "clr = classification_report(testImages.labels, predictions, labels=[0, 1], target_names=[\"CAT\", \"DOG\"])\n",
    "print(clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def07ace",
   "metadata": {},
   "source": [
    "[Click this link to return to the top of the page](#sections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
